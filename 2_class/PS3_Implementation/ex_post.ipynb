{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(legacy='1.25')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3: Implementer manuelt OLS-estimatoren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dette Problem Set skal vi udvide vores forståelse af OLS-estimatoren ved selv at implementere den i Python. Til sidst vil vi stå med en hjemmelavet implementering, som i princippet kan anvendes (og udvides) i resten af kurset i stedet for at bruge `statsmodels`. \n",
    "\n",
    "Ugens opgaver kan løses ved blot at implementere de formler, der forsynes undervejs (og som I har udledt og studeret i forelæsningerne).\n",
    "\n",
    "Går man i stå, kan man kigge i forelæsningskoden, hvor OLS-estimatoren også implementeres manuelt – men der er et stort læringsudbytte i at prøve at implementere den selv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undervejs i opgaverne kan du tjekke, at din implementering fungerer korrekt ved at køre din funktion på følgende simple testdata for $n=8$ observationer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0.67, 0.03, -0.78,  0.95,  0.7,  -1.05, -0.37, -1.14])\n",
    "y = np.array([-2.31, 2.6, -3.25, 2.24, 1.05, -2.98, -3.82, -5.29])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Del 1: OLS i SLR-tilfældet (en enkelt forklarende variabel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Når vi kun har 1 forklarende variabel og en konstant (SLR-tilfældet) arbejder vi med denne model:\n",
    "\n",
    "$$ y_i = \\beta_0 + \\beta_1 x_i + u_i $$\n",
    "\n",
    "hvor $y_i$ er den afhængige variabel, $x_i$ er den forklarende variabel, $u_i$ er fejlledet, og $\\beta_0$ og $\\beta_1$ er modelparametre. Vi observerer kun $x$ og $y$, mens fejlledet og parametrene er ukendte.\n",
    "\n",
    "Såfremt vores model er korrekt specificeret, da ved vi fra den økonometriske teori, at den sande værdi af $\\beta_1$ er givet ved:\n",
    "\n",
    "$$ \\beta_1 = \\frac{{\\text{Cov}}(x, y)} {\\text{Var}(x)}  $$\n",
    "\n",
    "Vi ved også fra forelæsningerne, at vi med en stikprøve at $x,y$ observationer kan estimere kovariansen og variansen sådan her:\n",
    "\n",
    "\\begin{align*}\n",
    "\\widehat{\\text{Cov}}(x,y)  = \\frac{1}{n} \\sum (x_i - \\bar{x})(y_i - \\bar{y}) \\\\\n",
    "\\widehat{\\text{Var}}(x) = \\frac{1}{n} \\sum (x_i - \\bar{x})^2\n",
    "\\end{align*}\n",
    "\n",
    "hvor $\\bar{x}$ og $\\bar{y}$ er de empiriske gennemsnit af $x, y$ i vores datasæt.  Sætter vi alt dette sammen ved hjælp af Analogi-princippet, betyder det at vi kan estimere $\\hat{\\beta}_1$ med formlen:\n",
    "$$\n",
    "\\hat{\\beta}_1 = \\frac{{\\widehat{\\text{Cov}}}(x, y)} {\\widehat {\\text{Var}}(x)}  = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y}) }{ \\sum (x_i - \\bar{x})^2} \\tag{1}\n",
    "$$\n",
    "\n",
    "Dette er OLS-estimatoren for $\\hat{\\beta}_1$ i SLR-tilfældet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opgave 1.1\n",
    "Skriv en funktion `OLS_SLR()`, hvor du manuelt implementerer OLS-estimatoren for $\\hat{\\beta}_1$ i SLR-tilfældet. Brug NumPy i din implementering. Funktionen skal tage $x$ og $y$ som argumenter, hvor $x$ er et $(n \\times 1)$ NumPy-array og $y$ er et $(n \\times 1)$ NumPy-array. Funktionen skal returnere skalar-værdien $\\hat \\beta_1$. \n",
    "\n",
    "Når du kører din funktion på testdataen, bør du få estimatet $\\hat\\beta_1 = 2.68...$\n",
    "\n",
    "_Hints:_ \n",
    "- Du kan bruge NumPy-funktionen `np.mean()` til at beregne gennemsnittet af et NumPy-array og `np.sum()` til at beregne summen over et NumPy-array.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Din kode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.680398073836276"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def OLS_SLR(x,y):\n",
    "    x_bar = np.mean(x)\n",
    "    y_bar = np.mean(y)\n",
    "\n",
    "    cov_xy = np.sum((x - x_bar)*(y - y_bar)) \n",
    "    var_x  = np.sum((x - x_bar)**2)\n",
    "\n",
    "    beta1hat = cov_xy / var_x\n",
    "\n",
    "    return beta1hat\n",
    "\n",
    "\n",
    "OLS_SLR(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Fra den økonometriske teori ved vi også, at vi kan estimere interceptet $\\beta_0$ med formlen:\n",
    "$$ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} $$\n",
    "\n",
    "Reflekter et øjeblik over formlen:  Interceptet $\\hat \\beta_0$ er skæringspunktet på $y$-aksen, dvs. vores forventning til $y$ når $x$ er lig 0. Vi finder interceptet ved at tage gennemsnittet over vores $y$-observationer, dvs. $\\overline y$, og så fratrække den estimerede effekt af den gennemsnitlige $x$-observation, dvs. $\\hat \\beta_1 \\overline x$. \n",
    "\n",
    "Hvis du virkelig vil overbevise dig selv om intuitionen i formlen, så tegn et tilfældigt scatter plot på et stykke papir, tegn den bedste lige linje (en OLS-linje) igennem observationerne og se selv hvor linjen skærer $y$-aksen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opgave 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Udvid din `OLS_SLR()` funktion så den returnerer ikke bare $\\hat{\\beta}_1$, men en tuple der indeholder ${(\\hat\\beta_0, \\hat\\beta_1)}$. Når du kører din udvidede funktion på samme testdata som før, bør du få estimaterne ${(\\hat\\beta_0 = -1.13..., \\hat\\beta_1 = 2.68...)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.1383007383627606, 2.680398073836276)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def OLS_SLR(x,y):\n",
    "    x_bar = np.mean(x)\n",
    "    y_bar = np.mean(y)\n",
    "    n = len(x)\n",
    "\n",
    "    cov_xy = np.sum((x - x_bar)*(y - y_bar)) / n\n",
    "    var_x  = np.sum((x - x_bar)**2) / n\n",
    "\n",
    "    beta1hat = cov_xy / var_x\n",
    "    beta0hat = y_bar - beta1hat*x_bar\n",
    "    \n",
    "    return (beta0hat, beta1hat)\n",
    "\n",
    "OLS_SLR(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opgave 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi skal nu beregne de prædikterede værdier $\\hat y$ på baggrund af vores parameterestimater. Udvid din funktion, så den også returnerer et $(n \\times 1)$-NumPy indeholdene $\\hat y$, dvs:\n",
    "\n",
    "$$ \\hat{y} = \\hat \\beta_0 + \\hat \\beta_1 x $$\n",
    "\n",
    "Funktionen skal altså returnere tuplen $(\\hat \\beta_0, \\hat \\beta_1, \\hat y)$, hvor $\\hat \\beta_0$ er en skalar, $\\hat \\beta_1$ er en skalar og $\\hat y$ er en vektor (siden der hører én prædikteret værdi til hver observation af $x$). Hvis du har implementeret funktionen korrekt, skulle du gerne få at \n",
    "$${\\hat\\beta_0 = -1.13..., \\quad  \\hat\\beta_1 = 2.68...}$$\n",
    "$$\\hat y = (0.65..., -1.05..., -3.22...,  1.40..., 0.73...,\n",
    "        -3.95..., -2.13..., -4.19...)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Din kode her**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.1383007383627606,\n",
       " 2.680398073836276,\n",
       " array([ 0.65756597, -1.0578888 , -3.22901124,  1.40807743,  0.73797791,\n",
       "        -3.95271872, -2.13004803, -4.19395454]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def OLS_SLR(x,y):\n",
    "    x_bar = np.mean(x)\n",
    "    y_bar = np.mean(y)\n",
    "    n = len(x)\n",
    "\n",
    "    cov_xy = np.sum((x - x_bar)*(y - y_bar)) / n\n",
    "    var_x  = np.sum((x - x_bar)**2) / n\n",
    "\n",
    "    beta1hat = cov_xy / var_x\n",
    "    beta0hat = y_bar - beta1hat*x_bar\n",
    "\n",
    "    yhat = beta0hat + beta1hat*x\n",
    "    \n",
    "    return (beta0hat, beta1hat, yhat)\n",
    "\n",
    "OLS_SLR(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siden OLS-linjen er defineret ved vores prædiktioner $\\hat y$, har vi nu  alt, hvad vi behøver for at kunne tegne vores OLS-linje i et scatter plot af vores (x,y)-observationer.\n",
    "\n",
    "Kør nedenstående celle for at se resultatet. \n",
    "\n",
    "(Bemærk, at vi udpakker outputtet fra funktion med koden `beta0hat, beta1hat, yhat = OLS_SLR(X,Y)`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "beta0hat, beta1hat, yhat = OLS_SLR(x,y)\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.plot(x, yhat, color='red', marker='.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opgave 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Betragt ovenstående plot og tjek, at du har styr på den basale intuition. Hvor på figuren kan du aflæse $\\hat y$? Hvad med $\\hat \\beta_0$ og $\\hat \\beta_1$? Hvor er residualerne $\\hat u_i$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dit svar her**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> - $\\hat y$ aflæses som de små punkter langs den røde OLS-linjen, som implicit definerer OLS-linjen. Der hører én prædiktion til hvert observeret $x$-værdi: $\\hat y_i = \\hat \\beta_0 + \\hat \\beta_1 x_i$.\n",
    "> - $\\hat \\beta_0$ aflæses som skæringspunktet mellem den røde OLS-linje og $y$-aksen (der hvor $x=0$).\n",
    "> - $\\hat \\beta_1$ aflæses som den røde OLS-linjes hældningskoefficient\n",
    "> - Residualerne $\\hat u_i$ aflæses som den vertikale afstand mellem hver observation $(x,y)$ og den tilhørende prædiktion $(x,\\hat y)$, det vil sige forskellen $y - \\hat y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opgave 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan implicit aflæse residualerne af figuren fra forrige opgave, men lad os udvide vores funktion, så den også beregner og returnerer residualerne. Residualerne skal vi senere bruge til at beregne modellens forklaringsgrad og til at beregne standardfejl til vores parameterestimater.\n",
    "\n",
    "Residualerne er den del af $y$, som vores model ikke forklarer, dvs:\n",
    "\n",
    "\\begin{align*} \n",
    "\\hat u &= y - \\hat y \\\\\n",
    "&=  y - \\hat \\beta_0 - \\hat \\beta_1 x\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Husk, at der er én residual til hver $(x,y)$ observation, så outputtet af din funktion skal nu rumme et ekstra NumPy-array bestående af $(n \\times 1)$-elementer.\n",
    "\n",
    "Hvis du har løst opgaven korrekt, skal du få residualerne\n",
    "$$\n",
    "\\hat u = (-2.96...,  3.65... , -0.02...,  0.83...,  0.31...,\n",
    "         0.97..., -1.68..., -1.09...)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Din kode**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.1383007383627606,\n",
       " 2.680398073836276,\n",
       " array([ 0.65756597, -1.0578888 , -3.22901124,  1.40807743,  0.73797791,\n",
       "        -3.95271872, -2.13004803, -4.19395454]),\n",
       " array([-2.96756597,  3.6578888 , -0.02098876,  0.83192257,  0.31202209,\n",
       "         0.97271872, -1.68995197, -1.09604546]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def OLS_SLR(x,y):\n",
    "    x_bar = np.mean(x)\n",
    "    y_bar = np.mean(y)\n",
    "    n = len(x)\n",
    "\n",
    "    cov_xy = np.sum((x - x_bar)*(y - y_bar)) / n\n",
    "    var_x  = np.sum((x - x_bar)**2) / n\n",
    "\n",
    "    beta1hat = cov_xy / var_x\n",
    "    beta0hat = y_bar - beta1hat*x_bar\n",
    "\n",
    "    yhat = beta0hat + beta1hat*x\n",
    "    uhat = y - yhat\n",
    "    \n",
    "    return (beta0hat, beta1hat, yhat, uhat)\n",
    "\n",
    "OLS_SLR(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opgave 1.6:\n",
    "Det begynder at blive lidt svært at holde styr på alt det output, som vores funktion returnerer. Udvid din funktion, så den pænt printer hvert resultat med en deskriptiv titel, og gerne afrundet til et fornuftigt antal decimaler. Når du kører din funktion på testdataen, skal du gerne få et output i stil med:\n",
    "\n",
    "```plain\n",
    "beta0hat = -1.1383\n",
    "beta1hat = 2.6804\n",
    "yhat = [ 0.66 -1.06 -3.23  1.41  0.74 -3.95 -2.13 -4.19]\n",
    "uhat = [-2.97  3.66 -0.02  0.83  0.31  0.97 -1.69 -1.1 ]\n",
    "```\n",
    "\n",
    "_Hint:_ Her kan det være en god ide at bruge en f-string i Python. Se fx følgende eksempel på, hvordan du let kan printe og formatere variable ved hjælp af f-strings:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "abc = 123.456789\n",
    "\n",
    "print(f'Min variabels værdi er: {abc}')\n",
    ">> Min variabels værdi er: 123.456789\n",
    "\n",
    "print(f'Min variabels navn og værdi er: {abc = }')\n",
    ">> Min variabels navn og værdi er: abc = 123.456789\n",
    "\n",
    "print(f'Min variabels navn og afrundede værdi er: {abc = :.2f}')\n",
    ">> Min variabels navn og afrundede værdi er: abc = 123.46\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta0hat = -1.1383\n",
      "beta1hat = 2.6804\n",
      "yhat = [ 0.66 -1.06 -3.23  1.41  0.74 -3.95 -2.13 -4.19]\n",
      "uhat = [-2.97  3.66 -0.02  0.83  0.31  0.97 -1.69 -1.1 ]\n"
     ]
    }
   ],
   "source": [
    "def OLS_SLR(x,y):\n",
    "    x_bar = np.mean(x)\n",
    "    y_bar = np.mean(y)\n",
    "    n = len(x)\n",
    "\n",
    "    cov_xy = np.sum((x - x_bar)*(y - y_bar)) / n\n",
    "    var_x  = np.sum((x - x_bar)**2) / n\n",
    "\n",
    "    beta1hat = cov_xy / var_x\n",
    "    beta0hat = y_bar - beta1hat*x_bar\n",
    "\n",
    "    yhat = beta0hat + beta1hat*x\n",
    "    uhat = y - yhat\n",
    "\n",
    "    print(f'{beta0hat = :.4f}')\n",
    "    print(f'{beta1hat = :.4f}')\n",
    "    print(f'yhat = {yhat.round(2)}')\n",
    "    print(f'uhat = {uhat.round(2)}')\n",
    "\n",
    "OLS_SLR(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opgave 1.7\n",
    "Udvid din funktion, så den også beregner modellens forklaringsgrad $R^2$. Forklaringsgraden $R^2$ er defineret som den forklarede del af den totale variation i $y$:\n",
    "\n",
    "\\begin{align*} \n",
    "    R^2 &\\equiv \\frac{SSE}{SST} \\\\ \n",
    "        &= 1 - \\frac{SSR}{SST}\n",
    "\\end{align*}\n",
    "\n",
    "hvor _Total Sum of Squares_ er\n",
    "\n",
    "$$\n",
    "SST = \\sum_{i=1}^n (y_i - \\overline{y})^2,\n",
    "$$\n",
    "\n",
    "mens _Explained Sum of Squares_ er\n",
    "\n",
    "$$\n",
    "SSE = \\sum_{i=1}^n (\\hat y_i - \\overline{y})^2,\n",
    "$$\n",
    "\n",
    "og _Sum of Squared Residuals_ er\n",
    "\n",
    "$$\n",
    "SSR = \\sum_{i=1}^n \\hat u_i^2. \\\\\n",
    "$$\n",
    "\n",
    "Læg mærke til, at per definition er $SSE \\leq SST$, og derfor er $R^2 \\in [0,1]$. Intutionen bag dette forhold er, at modellen aldrig kan forklare mere af variationen i $y$ end den totale variation.\n",
    "\n",
    "Du skulle gerne få resultatet\n",
    "\n",
    "$$\n",
    "R^2 = 0.5555\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Din kode her**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta0hat = -1.1383\n",
      "beta1hat = 2.6804\n",
      "yhat = [ 0.66 -1.06 -3.23  1.41  0.74 -3.95 -2.13 -4.19]\n",
      "uhat = [-2.97  3.66 -0.02  0.83  0.31  0.97 -1.69 -1.1 ]\n",
      "R2 = 0.5555\n"
     ]
    }
   ],
   "source": [
    "def OLS_SLR(x,y):\n",
    "    x_bar = np.mean(x)\n",
    "    y_bar = np.mean(y)\n",
    "    n = len(x)\n",
    "\n",
    "    cov_xy = np.sum((x - x_bar)*(y - y_bar)) / n\n",
    "    var_x  = np.sum((x - x_bar)**2) / n\n",
    "\n",
    "    beta1hat = cov_xy / var_x\n",
    "    beta0hat = y_bar - beta1hat*x_bar\n",
    "\n",
    "    yhat = beta0hat + beta1hat*x\n",
    "    uhat = y - yhat\n",
    "\n",
    "    SST = np.sum((y - y_bar)**2)\n",
    "    SSE = np.sum((yhat - y_bar)**2)\n",
    "    SSR = np.sum(uhat**2)\n",
    "\n",
    "    R2 = SSE / SST\n",
    "\n",
    "    print(f'{beta0hat = :.4f}')\n",
    "    print(f'{beta1hat = :.4f}')\n",
    "    print(f'yhat = {yhat.round(2)}')\n",
    "    print(f'uhat = {uhat.round(2)}')\n",
    "    print(f'{R2 = :.4f}')\n",
    "\n",
    "OLS_SLR(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det sidste trin er at beregne standardfejlene for vores OLS-estimater. \n",
    "\n",
    "Vi bruger først residualerne til at beregne residualvariansen $\\hat \\sigma^2$:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{n-2} \\cdot SSR = \\frac{1}{n-2} \\sum_{i=1}^{n} \\hat{u}_i^2\n",
    "$$\n",
    "\n",
    "som vi kan bruge til at beregne variansen af OLS-estimatorerne:\n",
    "$$\n",
    "\\text{Var}(\\hat{\\beta}_1 \\mid X) = \\frac{\\hat{\\sigma}^2}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\n",
    "$$\n",
    "$$\n",
    "\\text{Var}(\\hat{\\beta}_0 \\mid X) = \\frac{\\hat{\\sigma}^2 \\cdot \\frac{1}{n}\\sum_{i=1}^{n}x_{i}^{2}}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\n",
    "$$\n",
    "\n",
    "Standardfejlene er ganske enkelt kvadratrødderne af disse varianser:\n",
    "$$\n",
    "\\text{se}(\\hat{\\beta}_1) = \\sqrt{\\text{Var}(\\hat{\\beta}_1 \\mid X)}\n",
    "$$\n",
    "$$\n",
    "\\text{se}(\\hat{\\beta}_0) = \\sqrt{\\text{Var}(\\hat{\\beta}_0 \\mid X)}\n",
    "$$\n",
    "\n",
    "Hvis du har brug for en genopfriskning af, hvad standardfejlene betyder, og hvorfor de beregnes på denne måde, så er dette et godt tidspunkt at genbesøge ugens forelæsningsslides.\n",
    "\n",
    "(Bemærk, at vi her antager _homoskedasticitet_ (SLR.5). Vi antager altså, at residualvariansen er konstant og ikke fx er større, når $x$ er stor. Senere i kurset vil vi lære at estimere robuste standardfejl, som tager højde for eventuel heteroskedasticitet ved at anvende en udvidet formel for standardfejlene)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opgave 1.8\n",
    "Udvid din `OLS_SLR()` funktion så den også returnerer standardfejlene af koefficienterne. Når du kører funktionen på samme testdata som før, skulle du gerne få:\n",
    "\n",
    "$$ \\text{se}(\\hat \\beta_0) = 0.7730, \\quad \\text{se}(\\hat \\beta_1) = 0.9788  $$\n",
    "\n",
    "_Hint_: Du kan finde $n$ ved at bruge kommandoen `len(x)`.\n",
    "\n",
    "_Hint_: Brug funktionen `np.sqrt()` til at tage kvadratroden.\n",
    "\n",
    "_Hint_: Siden vi skal bruge størrelsen $\\sum_{i=1}^n (x_i - \\bar{x})^2$ flere gange undervejs, kan du med fordel gemme den som en variabel, som du fx kan kalde `SSTx`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta0hat = -1.1383\n",
      "beta1hat = 2.6804\n",
      "yhat = [ 0.66 -1.06 -3.23  1.41  0.74 -3.95 -2.13 -4.19]\n",
      "uhat = [-2.97  3.66 -0.02  0.83  0.31  0.97 -1.69 -1.1 ]\n",
      "R2 = 0.5555\n",
      "se_beta0hat = 0.7730\n",
      "se_beta1hat = 0.9788\n"
     ]
    }
   ],
   "source": [
    "def OLS_SLR(x,y):\n",
    "    x_bar = np.mean(x)\n",
    "    y_bar = np.mean(y)\n",
    "    n = len(x)\n",
    "\n",
    "    cov_xy = np.sum((x - x_bar)*(y - y_bar)) / n\n",
    "    var_x  = np.sum((x - x_bar)**2) / n\n",
    "\n",
    "    beta1hat = cov_xy / var_x\n",
    "    beta0hat = y_bar - beta1hat*x_bar\n",
    "\n",
    "    yhat = beta0hat + beta1hat*x\n",
    "    uhat = y - yhat\n",
    "\n",
    "    SST = np.sum((y - y_bar)**2)\n",
    "    SSE = np.sum((yhat - y_bar)**2)\n",
    "    SSR = np.sum(uhat**2)\n",
    "\n",
    "    R2 = SSE / SST\n",
    "\n",
    "    sigma2 = np.sum(uhat**2) * 1/(n-2)\n",
    "    SSTx = np.sum((x - x_bar)**2)\n",
    "\n",
    "    var_beta1 = sigma2 / SSTx\n",
    "    var_beta0 = sigma2 * np.mean(x**2) / SSTx\n",
    "\n",
    "    se_beta1hat = np.sqrt(var_beta1)\n",
    "    se_beta0hat = np.sqrt(var_beta0)\n",
    "\n",
    "    print(f'{beta0hat = :.4f}')\n",
    "    print(f'{beta1hat = :.4f}')\n",
    "    print(f'yhat = {yhat.round(2)}')\n",
    "    print(f'uhat = {uhat.round(2)}')\n",
    "    print(f'{R2 = :.4f}')\n",
    "    print(f'{se_beta0hat = :.4f}')\n",
    "    print(f'{se_beta1hat = :.4f}')\n",
    "\n",
    "\n",
    "OLS_SLR(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opgave 1.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tilpas afslutningsvist din funktion, så den i stedet for at printe alle resultaterne i stedet returnerer en dictionary, som indeholder alle de størrelser, vi har arbejdet med i delopgave 1 – dvs. parameterestimaterne, deres standardfejl, de prædikterede værdier, residualerne, $R^2$ samt SST, SSE og SSR.\n",
    "\n",
    "Ved at samle resultaterne i en dictionary kan man nemt slå op i outputtet og tilgå det element, man skal bruge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Din kode her**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beta0hat': -1.1383007383627606,\n",
       " 'beta1hat': 2.680398073836276,\n",
       " 'se_beta0hat': 0.7730380067399938,\n",
       " 'se_beta1hat': 0.9788330454642955,\n",
       " 'yhat': array([ 0.65756597, -1.0578888 , -3.22901124,  1.40807743,  0.73797791,\n",
       "        -3.95271872, -2.13004803, -4.19395454]),\n",
       " 'uhat': array([-2.96756597,  3.6578888 , -0.02098876,  0.83192257,  0.31202209,\n",
       "         0.97271872, -1.68995197, -1.09604546]),\n",
       " 'R2': 0.5555101205315474,\n",
       " 'SST': 62.94840000000001,\n",
       " 'SSE': 34.96847327126806,\n",
       " 'SSR': 27.979926728731943}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def OLS_SLR(x,y):\n",
    "    x_bar = np.mean(x)\n",
    "    y_bar = np.mean(y)\n",
    "    n = len(x)\n",
    "\n",
    "    cov_xy = np.sum((x - x_bar)*(y - y_bar)) / n\n",
    "    var_x  = np.sum((x - x_bar)**2) / n\n",
    "\n",
    "    beta1hat = cov_xy / var_x\n",
    "    beta0hat = y_bar - beta1hat*x_bar\n",
    "\n",
    "    yhat = beta0hat + beta1hat*x\n",
    "    uhat = y - yhat\n",
    "\n",
    "    SST = np.sum((y - y_bar)**2)\n",
    "    SSE = np.sum((yhat - y_bar)**2)\n",
    "    SSR = np.sum(uhat**2)\n",
    "\n",
    "    R2 = SSE / SST\n",
    "\n",
    "    sigma2 = np.sum(uhat**2) * 1/(n-2)\n",
    "    SSTx = np.sum((x - x_bar)**2)\n",
    "\n",
    "    var_beta1 = sigma2 / SSTx\n",
    "    var_beta0 = sigma2 * np.mean(x**2) / SSTx\n",
    "\n",
    "    se_beta1hat = np.sqrt(var_beta1)\n",
    "    se_beta0hat = np.sqrt(var_beta0)\n",
    "\n",
    "    return {\n",
    "        'beta0hat': beta0hat,\n",
    "        'beta1hat': beta1hat,\n",
    "        'se_beta0hat': se_beta0hat,\n",
    "        'se_beta1hat': se_beta1hat,\n",
    "        'yhat': yhat,\n",
    "        'uhat': uhat,\n",
    "        'R2': R2,\n",
    "        'SST': SST,\n",
    "        'SSE': SSE,\n",
    "        'SSR': SSR,\n",
    "        \n",
    "    }\n",
    "\n",
    "\n",
    "OLS_SLR(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opgave 1.10\n",
    "Brug `sm.OLS()` funktionen fra statsmodels-pakken til at estimere samme model på testdataen og bekræft at dine koefficienter, standardfejl og forklaringsgraden stemmer overens med dine egne resultater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.556\n",
      "Model:                            OLS   Adj. R-squared:                  0.481\n",
      "Method:                 Least Squares   F-statistic:                     7.499\n",
      "Date:                Sun, 07 Sep 2025   Prob (F-statistic):             0.0338\n",
      "Time:                        14:28:38   Log-Likelihood:                -16.360\n",
      "No. Observations:                   8   AIC:                             36.72\n",
      "Df Residuals:                       6   BIC:                             36.88\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -1.1383      0.773     -1.473      0.191      -3.030       0.753\n",
      "x1             2.6804      0.979      2.738      0.034       0.285       5.076\n",
      "==============================================================================\n",
      "Omnibus:                        0.851   Durbin-Watson:                   2.370\n",
      "Prob(Omnibus):                  0.653   Jarque-Bera (JB):                0.185\n",
      "Skew:                           0.348   Prob(JB):                        0.912\n",
      "Kurtosis:                       2.734   Cond. No.                         1.33\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "X = sm.add_constant(x)\n",
    "model = sm.OLS(y,X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Del 2: OLS i MLR-tilfældet (Når vi har flere forklarende variable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I det generelle tilfælde hvor vi har flere forklarende variable (MLR), ser modellen sådan her ud:\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 x_{1 i}+ \\beta_2 x_{2 i} +  \\dots + \\beta_k x_{k i} + u_i$$\n",
    "\n",
    "Da ved vi at OLS-estimatoren kan skrives på matrix-form ved følgende formel, som nok er den vigtigste formel at kunne i hele det her kursus:\n",
    "\n",
    "$$ {\\hat {\\beta}} = (X' X)^{-1} {X'} y $$\n",
    "\n",
    "Her gælder at:\n",
    "- $n$ er antallet af observationer. \n",
    "- $k$ er antallet af forklarende variable.\n",
    "- ${\\hat{\\beta}}$ er en $ (k+1) \\times 1 $ **vektor** af koefficientestimater (vi plusser $k$ med 1 for at gøre plads til konstantleddet)\n",
    "- $X$ er en $ n \\times (k+1) $ matrix indeholdende vores observationer af forklarende variable, hvor den første søjle er fyldt med 1-taller for at fange konstantledet.\n",
    "- $y$ er en $ n \\times 1 $ vektor som indeholder vores observationer af de afhængige variable\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opgave 1.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sammenlign OLS-estimatoren for $\\hat \\beta_1$ i SLR-tilfældet og for $ {\\hat \\beta}$ i MLR-tilfældet. Kig på tælleren og nævneren i de to formler. Kan du  genkende et mønster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SLR:\n",
    "$$ \\hat \\beta_1 = \\frac{{\\widehat {\\text{Cov}}}(x, y)} {\\widehat{\\text{Var}}(x)}  $$\n",
    "\n",
    "MLR:\n",
    "$$ \\hat {\\beta} = (X'X)^{-1} {X'} y $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dit svar:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> MLR formlen er en generalisering af SLR-formlen. Tælleren ${X'} y $ i MLR-tilfældet er analog til tælleren $\\widehat {\\text{Cov}(x,y)}$ i SLR-tilfældet. Tilsvarende er \"nævneren\" $X' X$ (læg mærke til at vi tager den inverse i formlen) i MLR-tilfældet analog til nævneren ${\\widehat{\\text{Var}}(x)}$ i SLR-tilfældet. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I de følgende opgaver får vi brug for noget ny data at teste vores funktion på, som rummer flere forklarende variable. Kør derfor nedenstående celle for at generere testdataen. Du behøver ikke at kunne forstå, hvad der foregår her:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = \n",
      "[[ 1.   -0.15 -0.22  0.84]\n",
      " [ 1.   -1.44 -0.74  0.86]\n",
      " [ 1.   -1.71 -0.24 -0.03]\n",
      " [ 1.   -0.63  0.04  1.4 ]\n",
      " [ 1.   -0.73  0.13  3.13]\n",
      " [ 1.    0.76  0.37  2.02]\n",
      " [ 1.    1.6   0.02  2.21]\n",
      " [ 1.   -0.41  1.27  1.34]]\n",
      "y = \n",
      "[-0.13 -0.35 -0.5   1.16 -0.03  0.89 -0.16  4.89]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "n = 8\n",
    "cov = np.array([\n",
    "    [1,.5,.25],\n",
    "    [.5,1,-.1],\n",
    "    [.25,-.1,1]]\n",
    ")\n",
    "\n",
    "X = np.random.multivariate_normal((0,.5,1), cov,n)\n",
    "ones = np.ones(n).reshape((n,1))\n",
    "X = np.hstack((ones, X))\n",
    "\n",
    "u = np.random.normal(0, 1, n)\n",
    "\n",
    "beta = np.random.normal(1, 1, 4)\n",
    "\n",
    "y = X @ beta + u\n",
    "# y = y.reshape((n,1))\n",
    "\n",
    "print('X = ')\n",
    "print(f'{X.round(2)}')\n",
    "\n",
    "print('y = ')\n",
    "print(f'{y.round(2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opgave 1.2\n",
    "Skriv en funktion `OLS()`, hvor du manuelt implementerer OLS estimatoren i MLR-tilfældet ved hjælp af NumPy. Funktionen skal tage $X$ og $y$ som argumenter, hvor $X$ er et $n \\times (k+1)$ NumPy-array og $y$ er et $n \\times 1$ NumPy-array. Funktionen skal returnere et $(k+1) \\times 1$ NumPy-array $\\hat{\\beta}$.\n",
    "\n",
    "_Hints:_ \n",
    "- Du kan gange matricer sammen i NumPy ved hjælp af @ operatoren. Hvis du for eksempel har to kompatible matricer A og B, kan du bruge koden `A @ B` til at beregne matrix-produktet. @ operatoren fungerer også til matrix-vektor produkter.\n",
    "\n",
    "- Du kan transponere en matrix i NumPy ved at tilføje `.T` til sidst. For eksempel returnerer `A.T` den transpose af matricen A.\n",
    "- Du kan invertere en matrix med `np.linalg.inv()` metoden. For eksempel returnerer `np.linalg.inv(A)` den inverse af matricen A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis du løser opgaven korrekt, skulle du gerne få estimaterne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\hat \\beta = \\begin{pmatrix} 0.86... \\\\ -0.23... \\\\ 3.05...\\\\  -0.31... \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Din kode:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.86510321 -0.23327914  3.05243859 -0.3189416 ]\n"
     ]
    }
   ],
   "source": [
    "def OLS(X,y):\n",
    "    betahat = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    return betahat\n",
    "\n",
    "print(OLS(X,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opgave 1.3\n",
    "Lad os ligesom før formatere funktionens output med nogle deskriptive titler, sådan at vi let kan aflæse, hvilken værdi der hører til $\\hat \\beta_0, \\hat \\beta_1$ og $\\hat \\beta_2$.\n",
    "\n",
    "_Hint:_ Du kan gøre dette ved at loope over indholdet af din `beta_hat`-vektor og printe hvert element med en f-string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta0: 0.8651\n",
      "beta1: -0.2333\n",
      "beta2: 3.0524\n",
      "beta3: -0.3189\n"
     ]
    }
   ],
   "source": [
    "def OLS(X,Y):\n",
    "    betahat = np.linalg.inv(X.T @ X) @ X.T @ Y\n",
    "    for i, beta in enumerate(betahat):\n",
    "        print(f'beta{i}: {beta :>.4f}')\n",
    "beta = OLS(X,y)\n",
    "beta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I MLR tilfældet kan $(n \\times 1)$-vektoren af prædikterede værdier $\\hat y$ beregnes ved formlen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*} \n",
    "\\hat{y} &\\equiv  \\hat \\beta_0 + \\hat \\beta_1 x_1 + \\hat \\beta_2 x_2 + ... + \\hat \\beta_k x_k \n",
    "\\\\ &= X {\\hat{\\beta}},\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hvor $X \\hat \\beta$ er et matrix-vektor produkt, idet $X$ er en matrix og $\\hat \\beta$ er en vektor.\n",
    "\n",
    "Vi kan også let beregne  $(n \\times 1)$-vektoren af residualer $\\hat u$ beregnes ved formlen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\\begin{align*} \n",
    "\\hat{u} &\\equiv  y - \\hat \\beta_0 - \\hat \\beta_1 x_1 - \\hat \\beta_2 x_2 - ... - \\hat \\beta_k x_k \n",
    "\\\\ &= y - X {\\hat{\\beta}},\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opgave 1.4\n",
    "Udvid din funktion, så den også beregner de prædikterede værdier og residualerne. Du skulle gerne få:\n",
    "\n",
    "$$\n",
    "\\hat y = (-0.02..., -1.31...,  0.54...,  0.69...,  0.43...,  1.18..., -0.15...,  4.41...)'\n",
    "$$\n",
    "og\n",
    "$$\n",
    " \\hat u = (-0.10...,  0.96..., -1.03...,  0.47..., -0.46..., -0.30..., -0.00...,  0.46...)'\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta0: 0.8651\n",
      "beta1: -0.2333\n",
      "beta2: 3.0524\n",
      "beta3: -0.3189\n",
      "yhat = [-0.025  -1.3168  0.5415  0.6921  0.4356  1.1886 -0.1524  4.4193]\n",
      "uhat = [-0.1022  0.9681 -1.0373  0.4728 -0.4612 -0.3005 -0.0067  0.467 ]\n"
     ]
    }
   ],
   "source": [
    "def OLS(X,y):\n",
    "    betahat = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    yhat = X@betahat\n",
    "    uhat = y - X @ betahat\n",
    "\n",
    "    for i, beta in enumerate(betahat):\n",
    "        print(f'beta{i}: {beta :>.4f}')\n",
    "\n",
    "    print(f'yhat = {yhat.round(4)}')\n",
    "    print(f'uhat = {uhat.round(4)}')\n",
    "\n",
    "OLS(X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opgave 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Udvid nu modellen, så den kan beregne forklaringsgraden $R^2$ samt SST, SSE og SSR. Alle disse formler de samme som i SLR-tilfældet (se opgave 1.7). \n",
    "\n",
    "Du skulle gerne få\n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "SST &= 22.25... \\\\\n",
    "SSE &= 19.48... \\\\\n",
    "SSR &= 2.76... \\\\\n",
    "R^2 &= 0.87... \\\\\n",
    "\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta0: 0.8651\n",
      "beta1: -0.2333\n",
      "beta2: 3.0524\n",
      "beta3: -0.3189\n",
      "yhat = [-0.025  -1.3168  0.5415  0.6921  0.4356  1.1886 -0.1524  4.4193]\n",
      "uhat = [-0.1022  0.9681 -1.0373  0.4728 -0.4612 -0.3005 -0.0067  0.467 ]\n",
      "SST = 22.2507\n",
      "SSE = 19.4825\n",
      "SSR = 2.7682\n",
      "R2 = 0.8756\n"
     ]
    }
   ],
   "source": [
    "def OLS(X,y):\n",
    "    betahat = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    yhat = X@betahat\n",
    "    uhat = y - X @ betahat\n",
    "\n",
    "    ybar = y.mean()\n",
    "\n",
    "    SST = np.sum((y - ybar)**2)\n",
    "    SSE = np.sum((yhat - ybar)**2)\n",
    "    SSR = np.sum(uhat**2)\n",
    "\n",
    "    R2 = SSE / SST\n",
    "\n",
    "\n",
    "    for i, beta in enumerate(betahat):\n",
    "        print(f'beta{i}: {beta :>.4f}')\n",
    "\n",
    "    print(f'yhat = {yhat.round(4)}')\n",
    "    print(f'uhat = {uhat.round(4)}')\n",
    "    print(f'{SST = :.4f}')\n",
    "    print(f'{SSE = :.4f}')\n",
    "    print(f'{SSR = :.4f}')\n",
    "    print(f'{R2 = :.4f}')\n",
    "\n",
    "OLS(X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For at beregne koefficientestimaternes standardfejl skal vi bruge residualvariansen $\\sigma^2$. Residualvariansen beregnes i MLR-tilfældet med formlen\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat \\sigma^2 &\\equiv \\frac{\\sum_{i=1}^n \\hat u_i^2}{n-(k+1)} \\\\\n",
    "&= \\frac {1}{n-k-1} \\hat u ' \\hat u.\n",
    "\\end{align*}\n",
    "\n",
    "Læg her mærke til det smart trick at $\\sum_{i=1}^n \\hat u_i^2$ svarer til prikproduktet $\\hat u ' \\hat u$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opgave 1.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Udvid din funktion, så den også beregner residualvariansen $\\hat \\sigma^2$. Du skulle gerne få at\n",
    "\n",
    "$$\n",
    "\\hat \\sigma^2 = 0.6922...\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Hint:_ Du skal bruge $n$ og $k$. Du finder $n$ som det første element i tuplen `X.shape`, mens $k + 1$ er det andet element. Vi husker nemlig at $X$ har dimensionerne $n \\times (k+1$) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Din kode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta0: 0.87\n",
      "beta1: -0.23\n",
      "beta2: 3.05\n",
      "beta3: -0.32\n",
      "yhat = [-0.03 -1.32  0.54  0.69  0.44  1.19 -0.15  4.42]\n",
      "uhat = [-0.1   0.97 -1.04  0.47 -0.46 -0.3  -0.01  0.47]\n",
      "R2 = 0.88\n",
      "sigma2 = 0.69\n"
     ]
    }
   ],
   "source": [
    "def OLS(X,y):\n",
    "    n = X.shape[0]\n",
    "    k = X.shape[1] - 1\n",
    "\n",
    "    betahat = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    yhat = X @ betahat\n",
    "    uhat = y - X @ betahat\n",
    "    \n",
    "    ybar = y.mean()\n",
    "\n",
    "    SST = np.sum((y - ybar)**2)\n",
    "    SSE = np.sum((yhat - ybar)**2)\n",
    "    SSR = np.sum(uhat**2)\n",
    "\n",
    "    R2 = SSE / SST\n",
    "    \n",
    "    sigma2 = np.dot(uhat, uhat) * 1/(n-k-1)\n",
    "\n",
    "    for i, beta in enumerate(betahat):\n",
    "        print(f'beta{i}: {beta :>.2f}')\n",
    "\n",
    "    print(f'yhat = {yhat.round(2)}')\n",
    "    print(f'uhat = {uhat.round(2)}')\n",
    "    print(f'R2 = {R2.round(2)}')\n",
    "    print(f'sigma2 = {sigma2.round(2)}')\n",
    "\n",
    "OLS(X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardfejlene $\\text{se}(\\hat \\beta)$ findes ved at tage kvadratroden af diagonalelementerne i kovariansmatricen $\\text{var}(\\widehat{\\beta} | X)$. Kovariansmatricen er givet ved\n",
    "\n",
    "$$\n",
    "\\text{var}(\\widehat{\\beta} | X) = \\hat \\sigma^2 (X'X)^{-1}\n",
    "$$\n",
    "\n",
    "Og standardfejlene beregnes altså så som:\n",
    "\n",
    "$$ \\text{se}(\\hat \\beta) = \\begin{pmatrix} \\text{se}(\\hat \\beta_0) \\\\ \\text{se}(\\hat \\beta_1) \\\\ \\vdots \\\\ \\text{se}(\\hat \\beta_k) \\end{pmatrix}\n",
    "$$\n",
    "hvor\n",
    "\n",
    "$$\\text{se}(\\hat \\beta_i) = \\sqrt{\\text{var}(\\widehat{\\beta} | X)_{i,i}} $$\n",
    "\n",
    "\n",
    "> Her er det måske en god ide at genbesøge hvad en kovariansmatrix er. Når vi har $k$ forklarende variable, kan koefficientestimaterne til hver variabel være korreleret med hinanden på forskellig vis. Fx kan det være, at $\\text{Cov}(\\hat \\beta_1, \\hat \\beta_2) >0$, $\\text{Cov}(\\hat \\beta_1,\\hat \\beta_3) < 0$ og $\\text{Cov}(\\hat \\beta_2, \\hat \\beta_3) = 0$. Alle disse kovarianser er indeholdt i kovariansmatricen, som  har dimensionerne $(k+1) \\times (k+1$). Skriver vi hele matricen op, får vi:\n",
    ">\n",
    "> $$\n",
    "\\text{var}(\\widehat{\\beta} | X) = \\hat \\sigma^2 (X'X)^{-1} = \n",
    "\\hat \\sigma^2\n",
    "\\begin{bmatrix}\n",
    "\\mathrm{Var}(\\hat{\\beta}_0) & \\mathrm{Cov}(\\hat{\\beta}_0,\\hat{\\beta}_1) & \\cdots & \\mathrm{Cov}(\\hat{\\beta}_0,\\hat{\\beta}_k) \\\\\n",
    "\\mathrm{Cov}(\\hat{\\beta}_1,\\hat{\\beta}_0) & \\mathrm{Var}(\\hat{\\beta}_1) & \\cdots & \\mathrm{Cov}(\\hat{\\beta}_1,\\hat{\\beta}_k) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\mathrm{Cov}(\\hat{\\beta}_k,\\hat{\\beta}_0) & \\mathrm{Cov}(\\hat{\\beta}_k,\\hat{\\beta}_1) & \\cdots & \\mathrm{Var}(\\hat{\\beta}_k)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " På diagonalen ligger variansen af hver koefficient, og tager vi kvadratroden af den, får vi standardfejlen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opgave 1.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Udvid din funktion, så den også beregner standardfejlene der hører til hvert parameterestimat. Du skulle gerne få\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\text{se}(\\hat \\beta) = \\begin{pmatrix} \\text{se}(\\hat \\beta_0) \\\\ \\text{se}(\\hat \\beta_1) \\\\ \\text{se}(\\hat \\beta_2) \\\\ \\text{se}(\\hat \\beta_3) \\end{pmatrix} \n",
    "\n",
    "= \\begin{pmatrix} 0.69... \\\\ 0.34... \\\\ 0.57... \\\\ 0.38... \\end{pmatrix} \n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Hint:_ Du skal først beregne kovariansmatricen, og derefter tage kvadratroden af diagonalelementerne.\n",
    "\n",
    "_Hint:_ For at tilgå diagonalelementerne af en matrix kan du bruge `np.diag()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta0: 0.87\n",
      "beta1: -0.23\n",
      "beta2: 3.05\n",
      "beta3: -0.32\n",
      "yhat = [-0.03 -1.32  0.54  0.69  0.44  1.19 -0.15  4.42]\n",
      "uhat = [-0.1   0.97 -1.04  0.47 -0.46 -0.3  -0.01  0.47]\n",
      "sigma2 = 0.69\n",
      "R2 = 0.88\n",
      "se_betahat = [0.6984 0.3483 0.5772 0.3885]\n"
     ]
    }
   ],
   "source": [
    "def OLS(X, y):\n",
    "    n = X.shape[0]\n",
    "    k = X.shape[1] - 1\n",
    "\n",
    "    betahat = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    yhat = X @ betahat\n",
    "    uhat = y - X @ betahat\n",
    "    \n",
    "    ybar = y.mean()\n",
    "\n",
    "    SST = np.sum((y - ybar)**2)\n",
    "    SSE = np.sum((yhat - ybar)**2)\n",
    "    SSR = np.sum(uhat**2)\n",
    "\n",
    "    R2 = SSE / SST\n",
    "    \n",
    "    sigma2 = np.dot(uhat, uhat) * 1/(n-k-1)\n",
    "\n",
    "    var_betahat = sigma2 * np.linalg.inv(X.T @ X)\n",
    "    se_betahat = np.sqrt(np.diag(var_betahat))\n",
    "\n",
    "    for i, beta in enumerate(betahat):\n",
    "        print(f'beta{i}: {beta :>.2f}')\n",
    "\n",
    "    print(f'yhat = {yhat.round(2)}')\n",
    "    print(f'uhat = {uhat.round(2)}')\n",
    "    print(f'sigma2 = {sigma2.round(2)}')\n",
    "    print(f'R2 = {R2.round(2)}')\n",
    "    print(f'se_betahat = {se_betahat.round(4)}')\n",
    "\n",
    "OLS(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opgave 1.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tilpas din funktion, så, den returnerer alt sit output i en dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'betahat': array([ 0.86510321, -0.23327914,  3.05243859, -0.3189416 ]),\n",
       " 'se_betahat': array([0.69843512, 0.34825192, 0.57720848, 0.38851086]),\n",
       " 'yhat': array([-0.02502439, -1.31684102,  0.54150628,  0.69214844,  0.43557645,\n",
       "         1.1885808 , -0.15244663,  4.41925414]),\n",
       " 'uhat': array([-0.10219524,  0.96805751, -1.03729278,  0.47279945, -0.46122037,\n",
       "        -0.30045765, -0.006664  ,  0.46697306]),\n",
       " 'sigma2': 0.6920505291059739,\n",
       " 'R2': 0.8755901389582822,\n",
       " 'SST': 22.250664804581987,\n",
       " 'SSE': 19.4824626881581,\n",
       " 'SSR': 2.768202116423895}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def OLS(X, y):\n",
    "    n = X.shape[0]\n",
    "    k = X.shape[1] - 1\n",
    "\n",
    "    betahat = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    yhat = X @ betahat\n",
    "    uhat = y - X @ betahat\n",
    "    \n",
    "    ybar = y.mean()\n",
    "\n",
    "    SST = np.sum((y - ybar)**2)\n",
    "    SSE = np.sum((yhat - ybar)**2)\n",
    "    SSR = np.sum(uhat**2)\n",
    "\n",
    "    R2 = SSE / SST\n",
    "    \n",
    "    sigma2 = np.dot(uhat, uhat) * 1/(n-k-1)\n",
    "\n",
    "    var_betahat = sigma2 * np.linalg.inv(X.T @ X)\n",
    "    se_betahat = np.sqrt(np.diag(var_betahat))\n",
    "\n",
    "    return {\n",
    "        'betahat': betahat,\n",
    "        'se_betahat': se_betahat,\n",
    "        'yhat': yhat,\n",
    "        'uhat': uhat,\n",
    "        'sigma2': sigma2,\n",
    "        'R2': R2,\n",
    "        'SST': SST,\n",
    "        'SSE': SSE,\n",
    "        'SSR': SSR,\n",
    "    }\n",
    "\n",
    "OLS(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opgave 1.9\n",
    "\n",
    "Tjek nu, at din `OLS()` funktionen fra MLR-tilfældet returnerer de samme koefficienter som i SLR-tilfældet når du kører den på SLR-testdataen. Kør bare de to celler  nedenfor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beta0hat': -1.1383007383627606,\n",
       " 'beta1hat': 2.680398073836276,\n",
       " 'se_beta0hat': 0.7730380067399938,\n",
       " 'se_beta1hat': 0.9788330454642955,\n",
       " 'yhat': array([ 0.65756597, -1.0578888 , -3.22901124,  1.40807743,  0.73797791,\n",
       "        -3.95271872, -2.13004803, -4.19395454]),\n",
       " 'uhat': array([-2.96756597,  3.6578888 , -0.02098876,  0.83192257,  0.31202209,\n",
       "         0.97271872, -1.68995197, -1.09604546]),\n",
       " 'R2': 0.5555101205315474,\n",
       " 'SST': 62.94840000000001,\n",
       " 'SSE': 34.96847327126806,\n",
       " 'SSR': 27.979926728731943}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SLR testdata\n",
    "x = np.array([0.67, 0.03, -0.78,  0.95,  0.7,  -1.05, -0.37, -1.14])\n",
    "y = np.array([-2.31, 2.6, -3.25, 2.24, 1.05, -2.98, -3.82, -5.29])\n",
    "\n",
    "OLS_SLR(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'betahat': array([-1.13830074,  2.68039807]),\n",
       " 'se_betahat': array([0.77303801, 0.97883305]),\n",
       " 'yhat': array([ 0.65756597, -1.0578888 , -3.22901124,  1.40807743,  0.73797791,\n",
       "        -3.95271872, -2.13004803, -4.19395454]),\n",
       " 'uhat': array([-2.96756597,  3.6578888 , -0.02098876,  0.83192257,  0.31202209,\n",
       "         0.97271872, -1.68995197, -1.09604546]),\n",
       " 'sigma2': 4.663321121455325,\n",
       " 'R2': 0.5555101205315475,\n",
       " 'SST': 62.94840000000001,\n",
       " 'SSE': 34.96847327126807,\n",
       " 'SSR': 27.979926728731947}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add constant\n",
    "ones = np.ones(n).reshape(n,1)\n",
    "X = np.hstack((ones, x.reshape(n,1)))\n",
    "\n",
    "OLS(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
